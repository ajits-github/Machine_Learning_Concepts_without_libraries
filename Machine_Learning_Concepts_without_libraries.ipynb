{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYY0tJtxtRiRSz9k5usadT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajits-github/Machine_Learning_Concepts_without_libraries/blob/main/Machine_Learning_Concepts_without_libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words(BoW)"
      ],
      "metadata": {
        "id": "D_y9CL8CKxsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will see with scikit learn."
      ],
      "metadata": {
        "id": "wMFVfzjjjsl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample movie reviews and their corresponding sentiments\n",
        "reviews = [\n",
        "    (\"I loved the movie, it was amazing!\", \"positive\"),\n",
        "    (\"The movie was terrible, I didn't like it.\", \"negative\"),\n",
        "    (\"The plot was intriguing and well-developed.\", \"positive\"),\n",
        "    (\"The acting was subpar and disappointing.\", \"negative\"),\n",
        "]\n",
        "\n",
        "# Separate text and labels\n",
        "texts, labels = zip(*reviews)\n",
        "\n",
        "# Create a Bag of Words representation using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Print the vocabulary (unique words)\n",
        "print(\"Vocabulary (Unique Words):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the Bag of Words representation\n",
        "print(\"\\nBag of Words Representation:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iOgY_n9K1w3",
        "outputId": "9c69688f-b3e1-475a-87df-2e812406f1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Unique Words):\n",
            "['acting' 'amazing' 'and' 'developed' 'didn' 'disappointing' 'intriguing'\n",
            " 'it' 'like' 'loved' 'movie' 'plot' 'subpar' 'terrible' 'the' 'was' 'well']\n",
            "\n",
            "Bag of Words Representation:\n",
            "[[0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0]\n",
            " [0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0]\n",
            " [0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1]\n",
            " [1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0]]\n",
            "\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X.toarray()\n",
        "X.get_shape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hOfvejkK7Qo",
        "outputId": "90092af5-94a6-4144-b6b2-4f0df2f2dfc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In bag of words, the words are tokenized and a dictionary is prepared from the data (including both -train and test). The dictionary is then ordered from A-Z alphabetically and a binary feature vector is prepared for each of the records present in the data. Hence, finally a matrix is obtained where each row's length is equal to number of words in the dictionary and 1 is inserted for the words which are found in that record while iterating through the dictionary of the words. So if total numer of records are 10 and total number of words after tokenization is 25, then the size of the matrix is 10*25 i.e. each row with 25 columns with values in {1,0}."
      ],
      "metadata": {
        "id": "9cjxlzJPN9Uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will try without using the library."
      ],
      "metadata": {
        "id": "Q7BHXXq-jy3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def count_words(text):\n",
        "    word_count = {}\n",
        "    tokens = tokenize(text)\n",
        "    for word in tokens:\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "    return word_count\n",
        "\n",
        "def create_bow(corpus):\n",
        "    word_index = {}\n",
        "    bow = []\n",
        "    for doc in corpus:\n",
        "        word_count = count_words(doc)\n",
        "        for word in word_count.keys():\n",
        "            if word not in word_index:\n",
        "                word_index[word] = len(word_index)\n",
        "        bow.append(word_count)\n",
        "    return bow, word_index\n",
        "\n",
        "def vectorize(bow, word_index):\n",
        "    num_documents = len(bow)\n",
        "    num_words = len(word_index)\n",
        "    vectorized_bow = np.zeros((num_documents, num_words), dtype=int)\n",
        "    for i, word_count in enumerate(bow):\n",
        "        for word, count in word_count.items():\n",
        "            word_idx = word_index[word]\n",
        "            vectorized_bow[i, word_idx] = count\n",
        "    return vectorized_bow\n",
        "\n",
        "# Sample text documents\n",
        "documents = [\n",
        "    \"Jane has 2 apples. She bought 3 more. How many apples does she have in total?\",\n",
        "    \"John also has apples. He picked 5 apples from the tree.\",\n",
        "    \"Jane and John both love apples.\"\n",
        "]\n",
        "\n",
        "# Create Bag of Words\n",
        "bow, word_index = create_bow(documents)\n",
        "\n",
        "# Vectorize the Bag of Words\n",
        "vectorized_bow = vectorize(bow, word_index)\n",
        "\n",
        "# Print the Bag of Words and word_index\n",
        "print(\"Bag of Words:\")\n",
        "print(vectorized_bow)\n",
        "print(\"\\nWord Index:\")\n",
        "print(word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6anMI5rj3EE",
        "outputId": "2efbe6b1-ed46-4374-c557-24838b59ca1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "[[1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0]\n",
            " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1]]\n",
            "\n",
            "Word Index:\n",
            "{'jane': 0, 'has': 1, '2': 2, 'apples.': 3, 'she': 4, 'bought': 5, '3': 6, 'more.': 7, 'how': 8, 'many': 9, 'apples': 10, 'does': 11, 'have': 12, 'in': 13, 'total?': 14, 'john': 15, 'also': 16, 'he': 17, 'picked': 18, '5': 19, 'from': 20, 'the': 21, 'tree.': 22, 'and': 23, 'both': 24, 'love': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF"
      ],
      "metadata": {
        "id": "1YFL2veNkfy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical representation used to represent the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "The TF-IDF weight for a word in a specific document is calculated using the following formula:\n",
        "\n",
        "TF-IDF = (Term Frequency in Document) * (Inverse Document Frequency)\n",
        "\n",
        "1. Term Frequency (TF): It represents how frequently a word appears in a document. It is calculated by dividing the number of times a word occurs in a document by the total number of words in that document.\n",
        "\n",
        "   TF = (Number of occurrences of word in document) / (Total number of words in document)\n",
        "\n",
        "2. Inverse Document Frequency (IDF): It represents the inverse proportion of documents that contain the word in the entire corpus. It is calculated by dividing the total number of documents in the corpus by the number of documents that contain the word, and then taking the logarithm of the result.\n",
        "\n",
        "   IDF = log((Total number of documents) / (Number of documents containing the word))\n",
        "\n",
        "The TF-IDF weight of a word is the product of TF and IDF, which gives a higher weight to words that appear frequently in a specific document but are rare in the rest of the corpus. This way, common words that appear in many documents (like \"the\" and \"and\") receive lower weights, while important and distinctive words in a specific document receive higher weights.\n",
        "\n",
        "The TF-IDF representation helps in capturing the uniqueness of each document in a corpus and is widely used in text mining, information retrieval, and natural language processing tasks."
      ],
      "metadata": {
        "id": "UyJrl_fPkvkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate TF-IDF without using scikit-learn, you can follow these steps and implement the calculation from scratch:\n",
        "* Calculate Term Frequency (TF) for each word in each document.\n",
        "* Calculate Document Frequency (DF) for each word in the entire corpus.\n",
        "* Calculate Inverse Document Frequency (IDF) for each word based on DF.\n",
        "* Compute the TF-IDF weight for each word in each document."
      ],
      "metadata": {
        "id": "C3Qb38UGkmLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_tf(word, document):\n",
        "    total_words = len(document.split())\n",
        "    word_count = document.split().count(word)\n",
        "    return word_count / total_words\n",
        "\n",
        "def calculate_df(word, corpus):\n",
        "    return sum(1 for doc in corpus if word in doc)\n",
        "\n",
        "def calculate_idf(word, corpus):\n",
        "    total_docs = len(corpus)\n",
        "    doc_freq = calculate_df(word, corpus)\n",
        "    return math.log(total_docs / (doc_freq + 1))  # Adding 1 to avoid division by zero\n",
        "\n",
        "def calculate_tfidf(word, document, corpus):\n",
        "    tf = calculate_tf(word, document)\n",
        "    idf = calculate_idf(word, corpus)\n",
        "    return tf * idf\n",
        "\n",
        "# Sample movie reviews and their corresponding sentiments\n",
        "reviews = [\n",
        "    \"I loved the movie, it was amazing!\",\n",
        "    \"The movie was terrible, I didn't like it.\",\n",
        "    \"The plot was intriguing and well-developed.\",\n",
        "    \"The acting was subpar and disappointing.\",\n",
        "]\n",
        "\n",
        "# Calculate TF-IDF for each word in each document\n",
        "corpus = reviews\n",
        "tfidf_matrix = []\n",
        "for doc in corpus:\n",
        "    doc_tfidf = {}\n",
        "    for word in doc.split():\n",
        "        doc_tfidf[word] = calculate_tfidf(word, doc, corpus)\n",
        "    tfidf_matrix.append(doc_tfidf)\n",
        "\n",
        "# Print the TF-IDF representation for each document\n",
        "for i, doc_tfidf in enumerate(tfidf_matrix):\n",
        "    print(f\"TF-IDF Representation for Document {i+1}:\")\n",
        "    for word, tfidf in doc_tfidf.items():\n",
        "        print(f\"{word}: {tfidf:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "srORWD-sketR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}